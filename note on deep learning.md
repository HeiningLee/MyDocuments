# 深度学习笔记
目前关于深度学习的主要知识，来源于Ian Goodfellow， Yoshua Bengio和Aaron Courville所著的Deep Learning一书。中译本是由北京大学数学科学院的张志华教授和一帮上海交大的CS研究生以及GitHub上的读者们完成的。翻译得很认真，可以说几乎已经到达了汉语的极限去做到深入浅出，是一本十分美好的书。

统计学是一种十分玄妙的东西，这是我这个初学者目前的感受。统计学为我们打开了一扇大门，在大门之外，是一个十分精彩而科学的概念世界。而我希望在学习完这本书之后，不仅能够在机器学习领域入门，而且能够在对于世界本质的理解上，更进一步。这就要求我在学习的过程中，记录自己关于这些观念的理解。  

# 1 深度学习基础
## 1.5 最大似然估计
在此之前，已经介绍了关于估计量的定义，以及关于估计质量的衡量标准（也就是无偏、有偏估计，还有均方根误差的视角，估计量的偏差要会证明）。那么问题来了，在衡量估计量的偏差之前，还有一个问题没有解决，那就是“怎样构造一个估计量”。目前常用的一个构造估计量的方法，是“最大似然估计”。

设有一组含有m个样本的数据集$X=\lbrace{x^{(1)},\cdots,x^{(m)}}\rbrace​$，独立地由总体的真实分布$p_{data}(x)​$生成。

机器学习的目标，是利用我们构造的一个模型分布$p_{model}{(x;\theta)}$去逼近这个真实分布$p_{data}(x)$. 而对参数$\theta$的估计$\hat{\theta}$，就是要令$p_{model}{(x;\theta)}$最大化：
$$
\begin{equation}
\begin{split}
\hat{\theta}_{ML}&=\mathop{\arg\max}_{\theta}p_{model}{(X;\theta)}\\&=\mathop{\arg\max}_{\theta}\prod_{i=1}^{m}p_{model}(x^{(i)};\theta)
\end{split}
\end{equation}
$$
所谓似然，到底是什么意思？看来似然就是可能性的意思，就是某个概率分布函数。这种表达定下来以后，再进行任何单调正相关的变换，其最大求值的运算都不会改变。

对上式两侧同时求对数，$\hat{\theta}_{ML}​$的值也不会变。

### 关于熵、交叉熵和KL散度

在最大似然估计的论述中，提到了交叉熵和KL散度的概念。这来源于知乎高赞回答。

#### 熵与交叉熵

熵，就是一个事件的信息量。一个基本的观念是，如果一个事件比较不可能发生，则关于它发生的消息（或者它发生这个事件）就含有较大的信息量。从信息论的观点，熵的意义是对A事件中的随机变量进行编码所需的最小字节数。（还记得Cybernetics中维纳的话吗？所谓信息，就是人类对自然界中所有“二中择一”的简单事件所做抉择的记录。这些基本决策，现在包含在“编码”中。）

事件A发生的熵定义为：
$$
S(A)=-\sum_iP_A{(x_i)}logP_A{(x_i)}
$$
这是事件A发生的“自信息量”。

如果衡量两个事件（消息）A和B互相之间的表达，用信息论的观点讲：当用事件B作为密码本来表示A时，所需要的“平均编码长度”，叫做A与B的交叉熵。定义式为：
$$
H(A,B)=-\sum_iP_A(x_i)logP_B(x_i)
$$
显然，交叉熵$H(A,B)$和$H(B,A)$所含的对数项不同，并且不可进一步简化。因此，交叉熵没有对称性。

#### 衡量两个事件（分布）之间的不同：KL散度

KL散度，一般作为衡量两个概率分布之间的差别的度量。定义式：
$$
\begin{split}
D_{KL}(A||B)&=\sum_iP_A(x_i)log\left[{\frac{P_A(x_i)}{P_B(x_i)}}\right]\\
&=\sum_iP_A(x_i)logP_A(x_i)-\sum_iP_A(x_i)logP_B(x_i)
\end{split}
$$
这是离散事件的情况，如果是连续事件，则写成积分形式：
$$
\begin{split}D_{KL}(A||B)&=\int_xP_A(x)log\left[{\frac{P_A(x)}{P_B(x)}}\right]\\&=\int_xP_A(x)logP_A(x)-\int_xP_A(x)logP_B(x)\end{split}
$$
特别地，当事件A固定时（目前还没有搞清楚，事件A固定或者另一种说法，S（A）为常量，究竟是指的什么情况），





由定义式可知，就像交叉熵一样，KL散度一般不作为超空间的距离概念来理解，因为它不具有对称性。而且如果按照定义式，KL散度是可以用熵和交叉熵表达的。
$$
D_{KL}(A||B)=-S(A)+H(A,B)
$$






但是，真实分布是不可知的，




